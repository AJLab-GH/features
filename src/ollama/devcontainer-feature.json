{
  "containerEnv": {
    "OLLAMA_HOST": "0.0.0.0"
  },
  "description": "Installs ollama",
  "documentationURL": "https://github.com/prulloac/devcontainer-features/tree/main/src/ollama",
  "entrypoint": "ollama serve",
  "id": "ollama",
  "installsAfter": [
    "ghcr.io/devcontainers/features/nvidia-cuda"
  ],
  "mounts": [
    {
      "source": "${devcontainerId}-ollama",
      "target": "/root/.ollama",
      "type": "volume"
    }
  ],
  "name": "ollama",
  "onCreateCommand": {
    "ollama-persistence-setup": "/usr/local/share/ollama/oncreate.sh"
  },
  "options": {
    "models": {
      "default": "",
      "description": "comma separated list of models to pull",
      "type": "string"
    }
  },
  "version": "0.0.2"
}
